import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from executor import ejecutar_plan  # ðŸ‘ˆ LangGraph

st.title("ChatWorks - Agente con Grafos y Estados")

# --------- MODELO ----------
@st.cache_resource # Esto hace que cargue mÃ¡s rÃ¡pido
def cargar_modelo():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = cargar_modelo()

# --------- FUNCIONES CON EJEMPLOS REALES ----------
funciones = [
    {
        "nombre": "buscar_producto",
        "ejemplos": ["Â¿CuÃ¡ntas lÃ¡mparas hay disponibles?", "Â¿QuÃ© precio tiene la mesa?", "Â¿Hay sillas en stock?", "MuÃ©strame los productos", "Â¿CuÃ¡nto cuesta el televisor?"]
    },
    {
        "nombre": "crear_pedido",
        "ejemplos": ["Quiero comprar una lÃ¡mpara", "Deseo ordenar una mesa", "Necesito adquirir una silla", "Quiero hacer un pedido"]
    },
    {
        "nombre": "buscar_cliente",
        "ejemplos": ["MuÃ©strame los datos del cliente Juan PÃ©rez", "Buscar cliente MarÃ­a LÃ³pez", "InformaciÃ³n del cliente Carlos SÃ¡nchez"]
    },
    {
        "nombre": "confirmar_pedido",
        "ejemplos": ["SÃ­, confirma el pedido", "Finalizar la compra", "Confirmar mi pedido", "Terminar pedido"]
    }
]

# --------- PREPARAR EMBEDDINGS ----------
frases = []
mapa_funciones = []
for f in funciones:
    for ejemplo in f["ejemplos"]:
        frases.append(ejemplo)
        mapa_funciones.append(f["nombre"])

embeddings_funciones = model.encode(frases)

# --------- LOGICA CONVERSACIONAL NATURAL ----------
def respuesta_natural_social(query):
    query = query.lower()
    if any(saludo in query for saludo in ["hola", "buenos dias", "buenas tardes"]):
        return "Â¡Hola! ðŸ‘‹ Bienvenido a ChatWorks. Â¿En quÃ© puedo ayudarte hoy?"
    elif any(animo in query for animo in ["como estas", "como vas", "que tal"]):
        return "Â¡Todo excelente por aquÃ­! ðŸ¤– Listos para procesar tus pedidos. Â¿Y tÃº, quÃ© tal va tu dÃ­a?"
    elif any(gracias in query for gracias in ["gracias", "agradecido"]):
        return "Â¡De nada! Es un placer ayudarte. Â¿Necesitas algo mÃ¡s?"
    else:
        return "Â¡Hola! No estoy seguro de entender esa solicitud, pero puedo ayudarte a buscar productos o crear pedidos. ðŸ˜Š"

# --------- SELECCION DE FUNCION ----------
def seleccionar_funcion(query, umbral=0.60):
    query_embedding = model.encode([query])
    scores = cosine_similarity(query_embedding, embeddings_funciones)[0]
    idx = scores.argmax()
    max_score = scores[idx]

    if max_score < umbral:
        return "conversacion", max_score
    return mapa_funciones[idx], max_score

# --------- INTERFAZ ----------
# --------- INTERFAZ ----------
query = st.text_input("Â¿QuÃ© necesitas?")

if query:
    # 1. LIMPIEZA Y PRIORIDAD SOCIAL (VÃ­a RÃ¡pida)
    query_minuscula = query.lower().strip()
    
    # Lista de frases sociales que queremos capturar SIEMPRE
    frases_sociales = ["como estas", "como vas", "que tal", "como va todo", "hola", "buenos dias"]
    
    if any(social in query_minuscula for social in frases_sociales):
        with st.expander("Ver logs del proceso (Interno)"):
            st.write("ðŸ’¬ Modo: InteracciÃ³n Social Directa")
            st.write("ðŸ§  Proceso: Respuesta natural activada por coincidencia de frase.")
        
        respuesta = respuesta_natural_social(query)
        st.info(respuesta)

    # 2. PROCESAMIENTO TÃ‰CNICO (Solo si no fue un saludo simple)
    else:
        funcion, score = seleccionar_funcion(query)

        # Mostrar logs de proceso (Punto 1 de la guÃ­a)
        with st.expander("Ver logs del proceso (Interno)"):
            st.write(f"ðŸ”Ž FunciÃ³n detectada: **{funcion}**")
            st.write(f"ðŸ“Š Score de confianza: **{score:.2f}**")
            st.write("ðŸ§  Proceso: GeneraciÃ³n de embeddings y bÃºsqueda semÃ¡ntica completada.")

        # RESPUESTA AL USUARIO BASADA EN EL SCORE
        if funcion == "conversacion":
            respuesta = respuesta_natural_social(query)
            st.info(respuesta)
        else:
            # Respuesta generada por el Grafo (Neo4j + LangGraph)
            with st.status("Ejecutando plan desde Neo4j..."):
                respuesta = ejecutar_plan(query, funcion)
            
            # PresentaciÃ³n natural del resultado
            st.success(respuesta)